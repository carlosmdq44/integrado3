Proyecto Integrador â€” Avances 1, 2 y 3
ğŸ¯ Objetivo General

DiseÃ±ar e implementar un pipeline ELT escalable que integre datos de mÃºltiples fuentes, los cargue en un Data Warehouse y los transforme en datasets listos para anÃ¡lisis de negocio.

El proyecto se desarrolla en tres entregas:

Avance 1: Pipeline ELT base con CSV Airbnb NYC â†’ DWH local (DuckDB).

Avance 2: RecolecciÃ³n desde APIs y Scraping, contenerizaciÃ³n con Docker, validaciÃ³n en capa raw.

Avance 3: Transformaciones avanzadas en SQL/Python, integraciÃ³n de datos no estructurados, validaciÃ³n de capas staging/core/gold.

ğŸ—ï¸ Avance 1 â€” Pipeline ELT + Data Warehouse

Fuentes:

CSV: AB_NYC.csv (Airbnb NYC dataset)

Pipeline:

Extract: copia de CSV a data/raw/airbnb/ con fecha.

Load: carga en raw.airbnb_listings (DuckDB).

Transform: limpieza en staging, modelo dimensional en core, KPIs en gold.

Data Warehouse (DuckDB):

raw â†’ crudo

staging â†’ limpio, tipificado

core â†’ hechos + dimensiones

gold â†’ datasets finales

Resultados principales:

gold.avg_price_by_area.csv

gold.room_type_offer.csv

gold.room_type_revenue_proxy.csv

gold.top_hosts.csv

gold.availability_by_district.csv

gold.reviews_monthly_by_ng.csv

Preguntas Q1â€“Q8 resueltas en notebooks/analisis_airbnb.ipynb
.

ğŸŒ Avance 2 â€” ExtracciÃ³n desde APIs y Scraping + Docker

Novedades:

Scripts Python parametrizados por YAML:

extract_api.py (APIs con requests + reintentos).

extract_scrape.py (web scraping con BeautifulSoup).

run_extract.py (ejecuta jobs definidos en config/extract_config.yaml).

ValidaciÃ³n de archivos raw (validate_raw.py) â†’ genera docs/raw_validation_report.md.

ConvenciÃ³n de nombres: fuente_fecha.json.

Manifest automÃ¡tico _manifest.csv.

Dockerfile:

Imagen base: python:3.10-slim

Instala requirements.txt

Copia scripts y config

ENTRYPOINT â†’ run_extract.py

Ejemplo de jobs:

jobs:
  - type: api
    name: httpbin_get_ip
    endpoint: https://httpbin.org/ip
  - type: scrape
    name: python_org_home
    url: https://www.python.org/


Resultados:

Archivos .json en data/raw/external/.

ValidaciÃ³n OK â†’ reporte con tamaÃ±o, formato y estado.

Imagen Docker lista para docker build y docker run.

ğŸ”„ Avance 3 â€” Transformaciones avanzadas + integraciÃ³n no estructurado

Objetivo: convertir datos crudos en informaciÃ³n Ãºtil para negocio, integrando fuentes estructuradas y no estructuradas.

Scripts principales:

normalize_external.py â†’

Convierte texto libre (name de listings) en features tabulares (has_wifi, has_pool, etc.).

Normaliza JSON externos (httpbin, scraping) en tablas staging.

sql_runner.py â†’ ejecuta transformaciones SQL.

sql/core_third.sql â†’ crea:

core.listing_text_features

core.fact_listings_enriched (con buckets de disponibilidad).

sql/gold_third.sql â†’ genera datasets de consumo:

gold.avg_price_by_area_room

gold.availability_bucket_by_ng

gold.corr_availability_reviews_by_ng

gold.price_vs_text_features

validate_transform.py â†’ checks automÃ¡ticos de existencia y consistencia.

Ejemplo de nuevas mÃ©tricas:

Impacto de has_wifi o is_luxury en el precio promedio.

CorrelaciÃ³n entre disponibilidad y cantidad de reseÃ±as por distrito.

DistribuciÃ³n de disponibilidad anual por buckets (0â€“60, 61â€“180, etc.).

Resultados:

ValidaciÃ³n final: OK VALIDATE TRANSFORM âœ”

Nuevas tablas core + gold disponibles en data/warehouse.duckdb.

ğŸ“‚ Estructura consolidada del proyecto
elt_airbnb_nyc/
â”œâ”€ data/
â”‚  â”œâ”€ raw/               # CSV/JSON originales
â”‚  â”œâ”€ staging/           # CSV limpios / features tabulares
â”‚  â”œâ”€ core/              # modelo de negocio
â”‚  â”œâ”€ gold/              # datasets finales
â”‚  â””â”€ warehouse.duckdb   # DWH local
â”œâ”€ scripts/
â”‚  â”œâ”€ extract_*.py       # extracciÃ³n (APIs, scraping)
â”‚  â”œâ”€ normalize_external.py
â”‚  â”œâ”€ sql_runner.py
â”‚  â”œâ”€ validate_transform.py
â”‚  â””â”€ quality_checks.py
â”œâ”€ sql/                  # transformaciones SQL (core, gold)
â”œâ”€ config/               # extract_config.yaml
â”œâ”€ notebooks/            # analisis_airbnb.ipynb
â”œâ”€ docs/                 # documentaciÃ³n + reportes
â”œâ”€ Dockerfile            # extractor (avance 2)
â””â”€ run.py                # pipeline avance 1

âœ… Estado de entregas

Avance 1

 Pipeline ELT (CSV â†’ raw â†’ staging â†’ core â†’ gold)

 Respuestas Q1â€“Q8 en notebook

Avance 2

 ExtracciÃ³n APIs + Scraping (YAML configurable)

 ValidaciÃ³n capa raw + reporte

 ContenerizaciÃ³n con Docker

Avance 3

 Transformaciones avanzadas SQL/Python

 IntegraciÃ³n datos no estructurados (texto + JSON externos)

 ValidaciÃ³n de staging/core/gold
