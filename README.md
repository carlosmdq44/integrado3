# Proyecto Integrador â€” Avances 1, 2 y 3

## ğŸ¯ Objetivo General
DiseÃ±ar e implementar un **pipeline ELT** escalable que integre datos de mÃºltiples fuentes, los cargue en un **Data Warehouse** y los transforme en datasets listos para anÃ¡lisis de negocio.  

El proyecto se desarrolla en **tres entregas**:
1. **Avance 1:** Pipeline ELT base con CSV Airbnb NYC â†’ DWH local (DuckDB).  
2. **Avance 2:** RecolecciÃ³n desde **APIs y Scraping**, contenerizaciÃ³n con Docker, validaciÃ³n en capa raw.  
3. **Avance 3:** Transformaciones avanzadas en **SQL/Python**, integraciÃ³n de datos no estructurados, validaciÃ³n de capas staging/core/gold.  

---

## ğŸ—ºï¸ Diagrama General (Mermaid)
> GitHub renderiza Mermaid automÃ¡ticamente. Si tu visor no lo soporta, abajo hay una versiÃ³n ASCII.

```mermaid
flowchart LR
    %% ====== FUENTES ======
    subgraph FUENTES
      CSV[CSV: AB_NYC.csv]
      API[(APIs pÃºblicas/privadas)]
      SCR[Scraping Web]
    end

    %% ====== EXTRACCIÃ“N ======
    subgraph EXTRACCIÃ“N (Python)
      EX_API[extract_api.py\n+ requests + retries]
      EX_SCR[extract_scrape.py\n+ BeautifulSoup]
      RUN_YAML[run_extract.py\n(config/extract_config.yaml)]
    end

    %% ====== RAW ======
    subgraph RAW (data/raw)
      RAW_CSV[airbnb/ab_nyc_YYYYMMDD.csv]
      RAW_EXT[external/*.json\n+ _manifest.csv]
      VALID_RAW[validate_raw.py\nâ†’ docs/raw_validation_report.md]
    end

    %% ====== STAGING ======
    subgraph STAGING (DuckDB)
      STG_CLEAN[staging.airbnb_listings_clean\n(cleaning, types, winsor)]
      STG_TEXT[staging.listing_text_features\n(text -> features)]
      STG_EXT[staging.external_* (JSON normalizado)]
    end

    %% ====== CORE ======
    subgraph CORE (DuckDB)
      FACT[fact_listings]
      LTF[core.listing_text_features]
      AVB[core.dim_availability_bucket]
      FACT_ENR[core.fact_listings_enriched]
    end

    %% ====== GOLD ======
    subgraph GOLD (DuckDB + CSV)
      G1[gold.avg_price_by_area.csv]
      G2[gold.room_type_offer.csv]
      G3[gold.room_type_revenue_proxy.csv]
      G4[gold.top_hosts.csv]
      G5[gold.availability_by_district.csv]
      G6[gold.reviews_monthly_by_ng.csv]
      G7[gold.avg_price_by_area_room]
      G8[gold.availability_bucket_by_ng]
      G9[gold.corr_availability_reviews_by_ng]
      G10[gold.price_vs_text_features]
    end

    %% ====== CONSUMO ======
    subgraph CONSUMO
      NB[notebooks/analisis_airbnb.ipynb]
      BI[Dashboards/BI\n(Tableau/PowerBI/QuickSight)]
    end

    %% ====== ORQUESTACIÃ“N / DEVOPS ======
    subgraph ORQ_DEVOPS
      RUN[run.py (A1)]
      SQLRUN[sql_runner.py (A3)]
      NORM[normalize_external.py (A3)]
      QCHK[quality_checks.py]
      VAL_T[validate_transform.py (A3)]
      DK[(Dockerfile\nExtractor A2)]
      CI[GitHub Actions\nbuild/push image]
      AFX[(Airflow - futuro)]
    end

    %% FLUJOS
    CSV --> RAW_CSV
    API --> EX_API --> RUN_YAML --> RAW_EXT
    SCR --> EX_SCR --> RUN_YAML --> RAW_EXT
    RAW_EXT --> VALID_RAW
    RAW_CSV --> STG_CLEAN
    RAW_EXT --> STG_EXT
    STG_CLEAN --> FACT
    STG_CLEAN --> NORM --> STG_TEXT
    STG_TEXT --> LTF
    FACT --> FACT_ENR
    AVB --> FACT_ENR

    FACT_ENR --> G7 & G8 & G9 & G10
    FACT --> G1 & G2 & G3 & G4 & G5 & G6

    G1 & G2 & G3 & G4 & G5 & G6 & G7 & G8 & G9 & G10 --> NB
    G1 & G7 & G8 & G9 & G10 --> BI

    RUN --> STG_CLEAN
    SQLRUN --> CORE & GOLD
    QCHK --> CORE
    VAL_T --> GOLD
    DK --> RUN_YAML
    CI --> DK
    AFX -. futuro .- RUN_YAML
    AFX -. futuro .- SQLRUN
```

### Diagrama (ASCII â€“ Fallback)
```
FUENTES
  â”œâ”€ CSV: AB_NYC.csv
  â”œâ”€ APIs
  â””â”€ Scraping
        â”‚
        â–¼
EXTRACCIÃ“N (Python)
  â”œâ”€ extract_api.py (requests + retries)
  â”œâ”€ extract_scrape.py (BeautifulSoup)
  â””â”€ run_extract.py (config YAML)
        â”‚
        â–¼
RAW (data/raw)
  â”œâ”€ airbnb/ab_nyc_YYYYMMDD.csv
  â”œâ”€ external/*.json + _manifest.csv
  â””â”€ validate_raw.py â†’ docs/raw_validation_report.md
        â”‚
        â–¼
STAGING (DuckDB)
  â”œâ”€ staging.airbnb_listings_clean (cleaning, types, winsor)
  â”œâ”€ staging.listing_text_features (textoâ†’features)
  â””â”€ staging.external_* (JSON normalizado)
        â”‚
        â–¼
CORE (DuckDB)
  â”œâ”€ fact_listings
  â”œâ”€ core.listing_text_features
  â”œâ”€ core.dim_availability_bucket
  â””â”€ core.fact_listings_enriched
        â”‚
        â–¼
GOLD (DuckDB + CSV)
  â”œâ”€ avg_price_by_area.csv, room_type_offer.csv, ...
  â”œâ”€ avg_price_by_area_room
  â”œâ”€ availability_bucket_by_ng
  â”œâ”€ corr_availability_reviews_by_ng
  â””â”€ price_vs_text_features
        â”‚
        â–¼
CONSUMO
  â”œâ”€ notebooks/analisis_airbnb.ipynb
  â””â”€ Dashboards/BI (Tableau/PowerBI/QuickSight)

ORQUESTACIÃ“N / DEVOPS
  â”œâ”€ run.py (Avance 1)
  â”œâ”€ sql_runner.py / normalize_external.py (Avance 3)
  â”œâ”€ quality_checks.py / validate_transform.py
  â”œâ”€ Dockerfile (Extractor Avance 2)
  â”œâ”€ GitHub Actions (build/push image)
  â””â”€ Airflow (futuro)
```

---

## ğŸ—ï¸ Avance 1 â€” Pipeline ELT + Data Warehouse

**Fuente principal:**  
- CSV: `AB_NYC.csv` (Airbnb NYC dataset)

**Pipeline:**  
1) **Extract:** CSV â†’ `data/raw/airbnb/ab_nyc_YYYYMMDD.csv`  
2) **Load:** `raw.airbnb_listings` (DuckDB)  
3) **Transform:** staging (limpieza/tipos/winsor), core (modelo dimensional), gold (KPIs)  

**Capas DWH (DuckDB):**  
- `raw` â†’ crudo  
- `staging` â†’ limpio, tipificado, `price_winsor`  
- `core` â†’ hechos + dimensiones  
- `gold` â†’ datasets finales

**Resultados (gold):**  
- `avg_price_by_area.csv`  
- `room_type_offer.csv`  
- `room_type_revenue_proxy.csv`  
- `top_hosts.csv`  
- `availability_by_district.csv`  
- `reviews_monthly_by_ng.csv`  

**Notebook:** [`notebooks/analisis_airbnb.ipynb`](notebooks/analisis_airbnb.ipynb) con Q1â€“Q8.

---

## ğŸŒ Avance 2 â€” ExtracciÃ³n desde APIs y Scraping + Docker

**Scripts:**  
- `extract_api.py` (APIs, reintentos/timeout)  
- `extract_scrape.py` (BeautifulSoup, meta + links)  
- `run_extract.py` (lee `config/extract_config.yaml`)  
- `validate_raw.py` (reporte: `docs/raw_validation_report.md`)  

**Convenciones RAW:**  
- Nombres: `fuente_YYYYMMDDThhmmssZ.json` (UTF-8, `\n`)  
- Manifest: `data/raw/_manifest.csv` (source, type, path, bytes, sha256)

**Docker (extractor):**  
- Imagen base: `python:3.10-slim`  
- Instala `requirements.txt`  
- Copia `scripts/` y `config/`  
- `ENTRYPOINT` â†’ `run_extract.py`  

**Ejemplo de jobs (`config/extract_config.yaml`):**
```yaml
jobs:
  - type: api
    name: httpbin_get_ip
    endpoint: https://httpbin.org/ip
  - type: scrape
    name: python_org_home
    url: https://www.python.org/
```

---

## ğŸ”„ Avance 3 â€” Transformaciones avanzadas + integraciÃ³n no estructurado

**Objetivo:** integrar **no estructurado â†’ estructurado** y enriquecer **core** para nuevos **gold**.  

**Scripts clave:**  
- `normalize_external.py`  
  - Texto libre de `name` (listings) â†’ features: `has_wifi`, `has_pool`, `has_garden`, `is_luxury`, `near_subway`.  
  - Normaliza JSON externos a `staging.external_*`.  
- `sql_runner.py` (ejecuta SQL)  
- `sql/core_third.sql`  
  - `core.listing_text_features`  
  - `core.dim_availability_bucket`  
  - `core.fact_listings_enriched`  
- `sql/gold_third.sql`  
  - `gold.avg_price_by_area_room`  
  - `gold.availability_bucket_by_ng`  
  - `gold.corr_availability_reviews_by_ng`  
  - `gold.price_vs_text_features`  
- `validate_transform.py` (checks de existencia/coherencia)  

**Ejemplos de anÃ¡lisis nuevos:**  
- Impacto de `has_wifi` / `is_luxury` en precio.  
- CorrelaciÃ³n entre `availability_365` y `number_of_reviews` por distrito.  
- Buckets de disponibilidad (`0â€“60`, `61â€“180`, `181â€“300`, `301â€“365`).  

---

## ğŸ“‚ Estructura consolidada del proyecto

```
elt_airbnb_nyc/
â”œâ”€ data/
â”‚  â”œâ”€ raw/               # CSV/JSON originales
â”‚  â”œâ”€ staging/           # CSV limpios / features tabulares
â”‚  â”œâ”€ core/              # modelo de negocio
â”‚  â”œâ”€ gold/              # datasets finales
â”‚  â””â”€ warehouse.duckdb   # DWH local
â”œâ”€ scripts/
â”‚  â”œâ”€ extract_*.py       # extracciÃ³n (APIs, scraping)
â”‚  â”œâ”€ normalize_external.py
â”‚  â”œâ”€ sql_runner.py
â”‚  â”œâ”€ validate_transform.py
â”‚  â””â”€ quality_checks.py
â”œâ”€ sql/                  # transformaciones SQL (core, gold)
â”œâ”€ config/               # extract_config.yaml
â”œâ”€ notebooks/            # analisis_airbnb.ipynb
â”œâ”€ docs/                 # documentaciÃ³n + reportes
â”œâ”€ Dockerfile            # extractor (avance 2)
â””â”€ run.py                # pipeline avance 1
```

---

## âœ… Estado de entregas

- **Avance 1**  
  - [x] Pipeline ELT (CSV â†’ raw â†’ staging â†’ core â†’ gold)  
  - [x] Respuestas Q1â€“Q8 en notebook  

- **Avance 2**  
  - [x] ExtracciÃ³n APIs + Scraping (YAML configurable)  
  - [x] ValidaciÃ³n capa raw + reporte  
  - [x] ContenerizaciÃ³n con Docker  

- **Avance 3**  
  - [x] Transformaciones avanzadas SQL/Python  
  - [x] IntegraciÃ³n datos no estructurados (texto + JSON externos)  
  - [x] ValidaciÃ³n de staging/core/gold  
