# Proyecto Integrador â€” Avances 1, 2, 3 y 4

## ğŸ¯ Objetivo General
DiseÃ±ar e implementar un **pipeline ELT** escalable que integre datos de mÃºltiples fuentes, los cargue en un **Data Warehouse**, y lo orqueste con **Airflow**, asegurando calidad continua mediante **CI/CD en GitHub Actions**.  

El proyecto se desarrolla en **cuatro entregas principales**:

1. **Avance 1:** Pipeline ELT base con CSV Airbnb NYC â†’ DWH local (DuckDB).  
2. **Avance 2:** RecolecciÃ³n desde **APIs y Scraping**, contenerizaciÃ³n con Docker, validaciÃ³n en capa raw.  
3. **Avance 3:** Transformaciones avanzadas en **SQL/Python**, integraciÃ³n de datos no estructurados, validaciÃ³n de capas staging/core/gold.  
4. **Avance 4:** **OrquestaciÃ³n con Apache Airflow** y **CI/CD con GitHub Actions** para ejecuciÃ³n automÃ¡tica y publicaciÃ³n de imÃ¡genes Docker.

---

## ğŸ—ï¸ Diagrama General (ASCII â€“ Fallback)

FUENTES

â”œâ”€ CSV: AB_NYC.csv

â”œâ”€ APIs

â””â”€ Scraping

â”‚

â–¼
EXTRACCIÃ“N (Python)

â”œâ”€ extract_api.py

â”œâ”€ extract_scrape.py

â””â”€ run_extract.py (config YAML)

â”‚

â–¼

RAW (data/raw)

â”œâ”€ ab_nyc_YYYYMMDD.csv

â”œâ”€ external/.json + manifest.csv

â””â”€ validate_raw.py â†’ docs/raw_validation_report.md

â”‚
â–¼

STAGING (DuckDB)

â”œâ”€ airbnb_listings_clean

â”œâ”€ listing_text_features

â””â”€ external (JSON normalizado)

â”‚

â–¼

CORE (DuckDB)

â”œâ”€ fact_listings

â”œâ”€ dim_availability_bucket

â””â”€ fact_listings_enriched

â”‚

â–¼

GOLD (DuckDB + CSV)

â”œâ”€ avg_price_by_area.csv

â”œâ”€ room_type_offer.csv

â”œâ”€ availability_bucket_by_ng.csv

â”œâ”€ corr_availability_reviews_by_ng.csv

â””â”€ price_vs_text_features.csv

â”‚

â–¼

CONSUMO

â”œâ”€ notebooks/analisis_airbnb.ipynb

â””â”€ Dashboards BI

ORQUESTACIÃ“N / DEVOPS

â”œâ”€ Airflow (DAGs: pipeline_airbnb_runall.py)

â”œâ”€ Dockerfile + docker-compose.yml

â”œâ”€ GitHub Actions (CI/CD + GHCR)

â””â”€ quality_checks.py / validate_transform.py


---

## ğŸš€ Avance 1 â€” Pipeline ELT + Data Warehouse

**Fuente principal:**  
- CSV: `AB_NYC.csv` (Airbnb NYC dataset)

**Pipeline:**  
1. **Extract:** CSV â†’ `data/raw/airbnb/ab_nyc_YYYYMMDD.csv`  
2. **Load:** `raw.airbnb_listings` (DuckDB)  
3. **Transform:** staging (limpieza/tipos/winsor), core (modelo dimensional), gold (KPIs)  

**Capas DWH (DuckDB):**
- `raw` â†’ crudo  
- `staging` â†’ limpio, tipificado  
- `core` â†’ hechos + dimensiones  
- `gold` â†’ datasets finales listos para BI  

**Resultados (gold):**
- `avg_price_by_area.csv`  
- `room_type_offer.csv`  
- `room_type_revenue_proxy.csv`  
- `top_hosts.csv`  
- `availability_by_district.csv`  
- `reviews_monthly_by_ng.csv`  

ğŸ‘‰ Notebook: [`notebooks/analisis_airbnb.ipynb`](notebooks/analisis_airbnb.ipynb) con respuestas a Q1â€“Q8.

---

## ğŸŒ Avance 2 â€” ExtracciÃ³n desde APIs y Scraping + Docker

**Scripts principales:**
- `extract_api.py` â†’ llamadas a APIs (requests + retries).  
- `extract_scrape.py` â†’ scraping con BeautifulSoup.  
- `run_extract.py` â†’ lee `config/extract_config.yaml` y ejecuta jobs.  
- `validate_raw.py` â†’ genera reporte en `docs/raw_validation_report.md`.

**Docker:**
- Imagen base: `python:3.11-slim`  
- Instala `requirements.txt`  
- Copia `scripts/` y `config/`  
- `ENTRYPOINT` â†’ `run_extract.py`  

**Convenciones capa RAW:**
- Archivos nombrados como `fuente_TIMESTAMP.json`  
- Manifest global: `data/raw/_manifest.csv`  

---

## ğŸ”„ Avance 3 â€” Transformaciones Avanzadas

**Objetivo:**  
- Enriquecer **core** con datos derivados de texto + JSON externos.  
- Crear nuevas mÃ©tricas en **gold**.

**Scripts clave:**
- `normalize_external.py` â†’ features de texto (`has_wifi`, `is_luxury`, etc.).  
- `sql_runner.py` â†’ ejecuta SQL desde `/sql`.  
- `sql/core_third.sql` â†’ tablas enriquecidas en `core`.  
- `sql/gold_third.sql` â†’ nuevos outputs `gold`.  
- `validate_transform.py` â†’ asegura consistencia.  

**AnÃ¡lisis extra:**
- Impacto de amenities (wifi, piscina, etc.) en precio.  
- CorrelaciÃ³n entre disponibilidad y reseÃ±as.  
- Buckets de disponibilidad (0â€“60, 61â€“180, etc.).

---

## âš™ï¸ Avance 4 â€” OrquestaciÃ³n con Airflow + CI/CD

**Airflow (docker-compose en `/airflow`):**
- Servicios: `airflow-postgres`, `airflow-scheduler`, `airflow-webserver`.  
- Usuario admin creado automÃ¡ticamente (`admin/admin`).  
- DAG principal: `pipeline_airbnb_runall.py`  
  - Orquesta extract, load, transform, gold, quality.  
  - Schedule: `@daily`.  

**CI/CD (GitHub Actions):**
- Workflows en `.github/workflows/`:
  - `ci.yml` â†’ lint + parse DAGs.  
  - `pipeline-smoke.yml` â†’ test end-to-end con CSV pequeÃ±o.  
  - `docker-publish.yml` â†’ build & push imagen a **GHCR**.  
- Imagen disponible en:  
docker pull ghcr.io/carlosmdq44/integrador3-elt:latest

---

## ğŸ“‚ Estructura del Proyecto

elt_airbnb_nyc/
â”œâ”€ data/
â”‚ â”œâ”€ raw/ # CSV/JSON originales
â”‚ â”œâ”€ staging/ # datos limpios
â”‚ â”œâ”€ core/ # modelo dimensional
â”‚ â”œâ”€ gold/ # datasets listos para BI
â”‚ â””â”€ warehouse.duckdb
â”œâ”€ scripts/
â”‚ â”œâ”€ extract_*.py
â”‚ â”œâ”€ normalize_external.py
â”‚ â”œâ”€ sql_runner.py
â”‚ â”œâ”€ quality_checks.py
â”‚ â””â”€ validate_transform.py
â”œâ”€ sql/
â”‚ â”œâ”€ core_third.sql
â”‚ â””â”€ gold_third.sql
â”œâ”€ config/
â”‚ â””â”€ extract_config.yaml
â”œâ”€ airflow/
â”‚ â”œâ”€ docker-compose.yml
â”‚ â””â”€ dags/pipeline_airbnb_runall.py
â”œâ”€ notebooks/
â”‚ â””â”€ analisis_airbnb.ipynb
â”œâ”€ docs/
â”‚ â””â”€ raw_validation_report.md
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ run.py
â””â”€ .github/workflows/
â”œâ”€ ci.yml
â”œâ”€ pipeline-smoke.yml
â””â”€ docker-publish.yml

---

## âœ… Estado de Entregas

- **Avance 1**  
  âœ” Pipeline ELT end-to-end (CSV â†’ raw â†’ staging â†’ core â†’ gold)  
  âœ” Respuestas Q1â€“Q8  

- **Avance 2**  
  âœ” ExtracciÃ³n desde APIs/scraping  
  âœ” ValidaciÃ³n en capa raw  
  âœ” Dockerfile y ejecuciÃ³n contenerizada  

- **Avance 3**  
  âœ” Transformaciones SQL/Python avanzadas  
  âœ” IntegraciÃ³n datos no estructurados  
  âœ” Nuevas mÃ©tricas gold  

- **Avance 4**  
  âœ” OrquestaciÃ³n con Airflow  
  âœ” CI/CD con GitHub Actions  
  âœ” PublicaciÃ³n automÃ¡tica en GHCR  
